{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import tifffile as tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from a directory\n",
    "def load_data_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename), 'rb') as f:\n",
    "            image, label = pickle.load(f)\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Paths to your data directories\n",
    "train_dir = \"data/classification_dataset/train\"\n",
    "validation_dir = \"data/classification_dataset/validation\"\n",
    "test_dir = \"data/classification_dataset/test\"\n",
    "\n",
    "# Load the data\n",
    "train_images, train_labels = load_data_from_directory(train_dir)\n",
    "val_images, val_labels = load_data_from_directory(validation_dir)\n",
    "test_images, test_labels = load_data_from_directory(test_dir)\n",
    "\n",
    "# Convert data to tf.data.Dataset\n",
    "def create_tf_dataset(images, labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_tf_dataset(train_images, train_labels)\n",
    "validation_dataset = create_tf_dataset(val_images, val_labels)\n",
    "test_dataset = create_tf_dataset(test_images, test_labels)\n",
    "\n",
    "# Batch and shuffle the datasets\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 32, 32) 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(1):\n",
    "        print(images[i].shape, labels[i].numpy())\n",
    "        image = images[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        # Reshape or reorder image data if necessary (bands, height, width -> height, width, bands)\n",
    "        image = np.transpose(image, (1, 2, 0))  # (12, 32, 32) -> (32, 32, 12)\n",
    "\n",
    "        # Select RGB bands (e.g., bands 4, 3, 2 from Sentinel-2, adjust if needed)\n",
    "        rgb_image = np.stack([\n",
    "            image[:, :, 3],  # Band 4 (Red)\n",
    "            image[:, :, 2],  # Band 3 (Green)\n",
    "            image[:, :, 1]   # Band 2 (Blue)\n",
    "        ], axis=-1)\n",
    "\n",
    "        # Normalize the RGB image for saving\n",
    "        rgb_image = (rgb_image / np.max(rgb_image) * 255).astype(np.uint8)\n",
    "\n",
    "        # Save the image as a TIFF file\n",
    "        # tiff.imwrite('rgb_image.tiff', rgb_image)\n",
    "        # print(\"Image saved as 'rgb_image.tiff'\")\n",
    "\n",
    "        # Display the RGB image\n",
    "        plt.imshow(rgb_image)\n",
    "        plt.title(f\"Label: {label.numpy()}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use class weights in model to re-balance the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 12, 32, 32)\n",
      "(32,)\n",
      "(32, 32, 32, 12)\n"
     ]
    }
   ],
   "source": [
    "#Size of training data\n",
    "for image_batch, labels_batch in train_dataset.take(1):\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SHAPE = (32, 32, 32,12)\n",
    "\n",
    "def reshape_tensors(image_batch, labels_batch):\n",
    "    image_batch = tf.reshape(image_batch, shape=TARGET_SHAPE)\n",
    "    return image_batch, labels_batch\n",
    "\n",
    "train_dataset = train_dataset.map(reshape_tensors, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.map(reshape_tensors, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.map(reshape_tensors, num_parallel_calls=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 32, 12)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in train_dataset.take(1):\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 32\n",
    "img_width = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 12)),\n",
    "  layers.Conv2D(16, 12, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 12, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 12, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_1 (Rescaling)     (None, 32, 32, 12)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 16)        27664     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 16, 16, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 16, 16, 32)        73760     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 64)          294976    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 4, 4, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               131200    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 527,858\n",
      "Trainable params: 527,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 668/1673 [==========>...................] - ETA: 1:10 - loss: 0.3103 - accuracy: 0.8547"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  validation_data=validation_dataset,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envuni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
