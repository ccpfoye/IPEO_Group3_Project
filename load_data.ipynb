{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "# import tifffile as tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from a directory\n",
    "def load_data_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename), 'rb') as f:\n",
    "            image, label = pickle.load(f)\n",
    "            # Reshape or reorder image data if necessary (bands, height, width -> height, width, bands)\n",
    "            image = np.transpose(image, (1, 2, 0))\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Paths to your data directories\n",
    "train_dir = \"data/classification_dataset/train\"\n",
    "validation_dir = \"data/classification_dataset/validation\"\n",
    "test_dir = \"data/classification_dataset/test\"\n",
    "\n",
    "# Load the data\n",
    "train_images, train_labels = load_data_from_directory(train_dir)\n",
    "val_images, val_labels = load_data_from_directory(validation_dir)\n",
    "test_images, test_labels = load_data_from_directory(test_dir)\n",
    "\n",
    "# Convert data to tf.data.Dataset\n",
    "def create_tf_dataset(images, labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_tf_dataset(train_images, train_labels)\n",
    "validation_dataset = create_tf_dataset(val_images, val_labels)\n",
    "test_dataset = create_tf_dataset(test_images, test_labels)\n",
    "\n",
    "# Batch and shuffle the datasets\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 12) 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(1):\n",
    "        print(images[i].shape, labels[i].numpy())\n",
    "        image = images[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        # Reshape or reorder image data if necessary (bands, height, width -> height, width, bands)\n",
    "        #image = np.transpose(image, (1, 2, 0))  # (12, 32, 32) -> (32, 32, 12)\n",
    "\n",
    "        # Select RGB bands (e.g., bands 4, 3, 2 from Sentinel-2, adjust if needed)\n",
    "        rgb_image = np.stack([\n",
    "            image[:, :, 3],  # Band 4 (Red)\n",
    "            image[:, :, 2],  # Band 3 (Green)\n",
    "            image[:, :, 1]   # Band 2 (Blue)\n",
    "        ], axis=-1)\n",
    "        \n",
    "        # Normalize the RGB image for saving\n",
    "        rgb_image = (rgb_image / np.max(rgb_image) * 255).astype(np.uint8)\n",
    "\n",
    "        # Save the image as a TIFF file\n",
    "        # tiff.imwrite('rgb_image.tiff', rgb_image)\n",
    "        # print(\"Image saved as 'rgb_image.tiff'\")\n",
    "\n",
    "        # Display the RGB image\n",
    "        plt.imshow(rgb_image)\n",
    "        plt.title(f\"Label: {label.numpy()}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use class weights in model to re-balance the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 32, 12)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "#Size of training data\n",
    "for image_batch, labels_batch in train_dataset.take(1):\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SHAPE = (-1, 32, 32,12)\n",
    "\n",
    "def reshape_tensors(image_batch, labels_batch):\n",
    "    image_batch = tf.reshape(image_batch, shape=TARGET_SHAPE)\n",
    "    return image_batch, labels_batch\n",
    "\n",
    "#train_dataset = train_dataset.map(reshape_tensors, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#validation_dataset = validation_dataset.map(reshape_tensors, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#test_dataset = test_dataset.map(reshape_tensors, num_parallel_calls=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 32, 12)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in train_dataset.take(1):\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 32\n",
    "img_width = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Conv2D(16, 12, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 12, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 12, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.0\n"
     ]
    }
   ],
   "source": [
    "#Input to reshape is a tensor with 380928 values, but the requested shape has 393216\n",
    "\n",
    "test = 380928/(32*32*12)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/envuni/lib/python3.9/site-packages/keras/engine/training.py:3214\u001b[0m, in \u001b[0;36mModel.summary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \n\u001b[1;32m   3186\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[1;32m   3212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m-> 3214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3218\u001b[0m     )\n\u001b[1;32m   3219\u001b[0m layer_utils\u001b[38;5;241m.\u001b[39mprint_summary(\n\u001b[1;32m   3220\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3221\u001b[0m     line_length\u001b[38;5;241m=\u001b[39mline_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3226\u001b[0m     layer_range\u001b[38;5;241m=\u001b[39mlayer_range,\n\u001b[1;32m   3227\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1673/1673 [==============================] - 123s 73ms/step - loss: 0.2905 - accuracy: 0.8525 - val_loss: 0.5748 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1673/1673 [==============================] - 128s 77ms/step - loss: 0.2717 - accuracy: 0.8569 - val_loss: 0.6153 - val_accuracy: 0.5416\n",
      "Epoch 3/10\n",
      "1673/1673 [==============================] - 131s 79ms/step - loss: 0.2587 - accuracy: 0.8674 - val_loss: 0.5344 - val_accuracy: 0.5897\n",
      "Epoch 4/10\n",
      "1673/1673 [==============================] - 137s 82ms/step - loss: 0.2526 - accuracy: 0.8713 - val_loss: 0.5454 - val_accuracy: 0.5584\n",
      "Epoch 5/10\n",
      "1673/1673 [==============================] - 138s 83ms/step - loss: 0.2528 - accuracy: 0.8705 - val_loss: 0.5092 - val_accuracy: 0.5824\n",
      "Epoch 6/10\n",
      "1673/1673 [==============================] - 157s 94ms/step - loss: 0.2499 - accuracy: 0.8720 - val_loss: 0.5539 - val_accuracy: 0.5703\n",
      "Epoch 7/10\n",
      "1673/1673 [==============================] - 148s 89ms/step - loss: 0.2524 - accuracy: 0.8723 - val_loss: 0.4944 - val_accuracy: 0.6122\n",
      "Epoch 8/10\n",
      "1673/1673 [==============================] - 141s 84ms/step - loss: 0.2469 - accuracy: 0.8732 - val_loss: 0.5295 - val_accuracy: 0.5846\n",
      "Epoch 9/10\n",
      "1673/1673 [==============================] - 144s 86ms/step - loss: 0.2447 - accuracy: 0.8735 - val_loss: 0.5265 - val_accuracy: 0.5975\n",
      "Epoch 10/10\n",
      "1673/1673 [==============================] - 148s 89ms/step - loss: 0.2444 - accuracy: 0.8740 - val_loss: 0.5134 - val_accuracy: 0.6056\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  validation_data=validation_dataset,\n",
    "  epochs=epochs\n",
    ")\n",
    "\n",
    "#no_classweights_10_ephocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model as a `.keras` zip archive.\n",
    "model.save('no_classweights_10_epochs.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.5864792620670012, 1: 3.390866480871548}\n"
     ]
    }
   ],
   "source": [
    "def calculate_class_weights(dataset):\n",
    "    # Extract all labels from the dataset\n",
    "    all_labels = []\n",
    "    for _, labels in dataset:\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "    # Count the occurrences of each class\n",
    "    label_counts = Counter(all_labels)\n",
    "\n",
    "    # Total number of samples\n",
    "    total_samples = sum(label_counts.values())\n",
    "\n",
    "    # Calculate weights for each class\n",
    "    class_weights = {label: total_samples / (len(label_counts) * count) \n",
    "                     for label, count in label_counts.items()}\n",
    "\n",
    "    return class_weights\n",
    "\n",
    "# Calculate class weights for the train_dataset\n",
    "class_weights = calculate_class_weights(train_dataset)\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 12:53:10.132565: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1673/1673 [==============================] - 4548s 3s/step - loss: 0.4030 - accuracy: 0.7035 - val_loss: 0.3836 - val_accuracy: 0.8271\n",
      "Epoch 2/10\n",
      "1673/1673 [==============================] - 134s 80ms/step - loss: 0.3923 - accuracy: 0.7066 - val_loss: 0.3919 - val_accuracy: 0.8233\n",
      "Epoch 3/10\n",
      " 248/1673 [===>..........................] - ETA: 39:59 - loss: 0.3817 - accuracy: 0.7077"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  validation_data=validation_dataset,\n",
    "  epochs=epochs,\n",
    "  class_weight = class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('with_classweights_10_epochs.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do some visualizations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envuni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
